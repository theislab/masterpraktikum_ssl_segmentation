{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c1360f",
   "metadata": {},
   "source": [
    "# PyTorch model to predict spot annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3c1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "#import squidpy as sq\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1f3d9",
   "metadata": {},
   "source": [
    "## Create anndata object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2c91e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3111 × 39739\n",
       "    obs: 'nGene', 'nUMI', 'Sample', 'weeks', 'ChipBatch', 'ChipNr', 'Experiment_date', 'Experiment_procedure', 'Sequencing_date', 'Raw_reads', 'new_x', 'new_y', 'percent.mito', 'res.0.8'\n",
       "    obsm: 'spatial'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = pd.read_csv(\"filtered_ST_matrix_and_meta_data/filtered_matrix.tsv.gz\", compression=\"gzip\", delimiter=\"\\t\")\n",
    "counts_df = counts_df.transpose()\n",
    "counts_df.columns = counts_df.iloc[0]\n",
    "counts_df.drop(counts_df.index[0], inplace = True)\n",
    "adata = sc.AnnData(counts_df)\n",
    "meta = pd.read_csv(\"filtered_ST_matrix_and_meta_data/meta_data.tsv\", sep=\"\\t\")\n",
    "adata.obs = meta\n",
    "adata.obsm[\"spatial\"] = adata.obs[[\"new_x\", \"new_y\"]].copy().to_numpy()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1eacee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>Sample</th>\n",
       "      <th>weeks</th>\n",
       "      <th>ChipBatch</th>\n",
       "      <th>ChipNr</th>\n",
       "      <th>Experiment_date</th>\n",
       "      <th>Experiment_procedure</th>\n",
       "      <th>Sequencing_date</th>\n",
       "      <th>Raw_reads</th>\n",
       "      <th>new_x</th>\n",
       "      <th>new_y</th>\n",
       "      <th>percent.mito</th>\n",
       "      <th>res.0.8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1x17x20</th>\n",
       "      <td>666</td>\n",
       "      <td>1046</td>\n",
       "      <td>FH5_1000L3_CN20_C1</td>\n",
       "      <td>5</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN20</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>151117</td>\n",
       "      <td>35432474</td>\n",
       "      <td>16.92</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.018164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x17x21</th>\n",
       "      <td>891</td>\n",
       "      <td>1361</td>\n",
       "      <td>FH5_1000L3_CN20_C1</td>\n",
       "      <td>5</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN20</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>151117</td>\n",
       "      <td>35432474</td>\n",
       "      <td>17.06</td>\n",
       "      <td>20.95</td>\n",
       "      <td>0.007348</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x17x22</th>\n",
       "      <td>1138</td>\n",
       "      <td>1878</td>\n",
       "      <td>FH5_1000L3_CN20_C1</td>\n",
       "      <td>5</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN20</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>151117</td>\n",
       "      <td>35432474</td>\n",
       "      <td>17.02</td>\n",
       "      <td>22.02</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x18x18</th>\n",
       "      <td>766</td>\n",
       "      <td>1170</td>\n",
       "      <td>FH5_1000L3_CN20_C1</td>\n",
       "      <td>5</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN20</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>151117</td>\n",
       "      <td>35432474</td>\n",
       "      <td>17.99</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.021368</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1x18x19</th>\n",
       "      <td>536</td>\n",
       "      <td>820</td>\n",
       "      <td>FH5_1000L3_CN20_C1</td>\n",
       "      <td>5</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN20</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>151117</td>\n",
       "      <td>35432474</td>\n",
       "      <td>17.95</td>\n",
       "      <td>18.99</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19x9x11</th>\n",
       "      <td>1133</td>\n",
       "      <td>2330</td>\n",
       "      <td>FH9_1000L3_CN31_E2</td>\n",
       "      <td>9</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN31</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>160111</td>\n",
       "      <td>83186609</td>\n",
       "      <td>8.92</td>\n",
       "      <td>11.08</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19x9x12</th>\n",
       "      <td>865</td>\n",
       "      <td>1599</td>\n",
       "      <td>FH9_1000L3_CN31_E2</td>\n",
       "      <td>9</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN31</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>160111</td>\n",
       "      <td>83186609</td>\n",
       "      <td>8.98</td>\n",
       "      <td>12.01</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19x9x13</th>\n",
       "      <td>1265</td>\n",
       "      <td>2897</td>\n",
       "      <td>FH9_1000L3_CN31_E2</td>\n",
       "      <td>9</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN31</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>160111</td>\n",
       "      <td>83186609</td>\n",
       "      <td>8.88</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19x9x8</th>\n",
       "      <td>1343</td>\n",
       "      <td>2594</td>\n",
       "      <td>FH9_1000L3_CN31_E2</td>\n",
       "      <td>9</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN31</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>160111</td>\n",
       "      <td>83186609</td>\n",
       "      <td>8.95</td>\n",
       "      <td>8.02</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19x9x9</th>\n",
       "      <td>1474</td>\n",
       "      <td>2971</td>\n",
       "      <td>FH9_1000L3_CN31_E2</td>\n",
       "      <td>9</td>\n",
       "      <td>1000L3</td>\n",
       "      <td>CN31</td>\n",
       "      <td>151105</td>\n",
       "      <td>manual</td>\n",
       "      <td>160111</td>\n",
       "      <td>83186609</td>\n",
       "      <td>8.93</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0.024907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3111 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         nGene  nUMI              Sample  weeks ChipBatch ChipNr  \\\n",
       "1x17x20    666  1046  FH5_1000L3_CN20_C1      5    1000L3   CN20   \n",
       "1x17x21    891  1361  FH5_1000L3_CN20_C1      5    1000L3   CN20   \n",
       "1x17x22   1138  1878  FH5_1000L3_CN20_C1      5    1000L3   CN20   \n",
       "1x18x18    766  1170  FH5_1000L3_CN20_C1      5    1000L3   CN20   \n",
       "1x18x19    536   820  FH5_1000L3_CN20_C1      5    1000L3   CN20   \n",
       "...        ...   ...                 ...    ...       ...    ...   \n",
       "19x9x11   1133  2330  FH9_1000L3_CN31_E2      9    1000L3   CN31   \n",
       "19x9x12    865  1599  FH9_1000L3_CN31_E2      9    1000L3   CN31   \n",
       "19x9x13   1265  2897  FH9_1000L3_CN31_E2      9    1000L3   CN31   \n",
       "19x9x8    1343  2594  FH9_1000L3_CN31_E2      9    1000L3   CN31   \n",
       "19x9x9    1474  2971  FH9_1000L3_CN31_E2      9    1000L3   CN31   \n",
       "\n",
       "         Experiment_date Experiment_procedure  Sequencing_date  Raw_reads  \\\n",
       "1x17x20           151105               manual           151117   35432474   \n",
       "1x17x21           151105               manual           151117   35432474   \n",
       "1x17x22           151105               manual           151117   35432474   \n",
       "1x18x18           151105               manual           151117   35432474   \n",
       "1x18x19           151105               manual           151117   35432474   \n",
       "...                  ...                  ...              ...        ...   \n",
       "19x9x11           151105               manual           160111   83186609   \n",
       "19x9x12           151105               manual           160111   83186609   \n",
       "19x9x13           151105               manual           160111   83186609   \n",
       "19x9x8            151105               manual           160111   83186609   \n",
       "19x9x9            151105               manual           160111   83186609   \n",
       "\n",
       "         new_x  new_y  percent.mito  res.0.8  \n",
       "1x17x20  16.92  19.99      0.018164        0  \n",
       "1x17x21  17.06  20.95      0.007348        2  \n",
       "1x17x22  17.02  22.02      0.007455        0  \n",
       "1x18x18  17.99  18.00      0.021368        4  \n",
       "1x18x19  17.95  18.99      0.014634        0  \n",
       "...        ...    ...           ...      ...  \n",
       "19x9x11   8.92  11.08      0.019313        3  \n",
       "19x9x12   8.98  12.01      0.023139        0  \n",
       "19x9x13   8.88  13.01      0.018985        1  \n",
       "19x9x8    8.95   8.02      0.014649        3  \n",
       "19x9x9    8.93   9.04      0.024907        1  \n",
       "\n",
       "[3111 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253c8ce",
   "metadata": {},
   "source": [
    "## Create train and test set\n",
    "Create sets based on random indices\n",
    "\n",
    "Training data 80%\n",
    "\n",
    "Test data 20%\n",
    "\n",
    "CustomDataset contains two tensors: one tensor from the gene-spot matrix and one tensor containing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116e4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, labels, data, transform=None, target_transform=None):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns the tensor with data and corresponding label in a tuple\n",
    "        data_point = self.data[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        if self.transform:\n",
    "            data_point = self.transform(data_point).float()\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data_point, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b685002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_2112\\2375660767.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  training_data = CustomDataset(labels = adata.obs[\"res.0.8\"][train_indices], data = training_matrix.astype(float),\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_2112\\2375660767.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  test_data = CustomDataset(labels = adata.obs[\"res.0.8\"][test_indices], data = test_matrix.astype(float),\n"
     ]
    }
   ],
   "source": [
    "test_indices = np.random.choice(len(adata.X), size=int(len(adata.X)*0.2), replace=False) \n",
    "train_indices = np.setdiff1d(np.arange(0, len(adata.X)), test_indices)\n",
    "\n",
    "training_matrix = adata.X[train_indices,:]\n",
    "test_matrix = adata.X[test_indices,:]\n",
    "\n",
    "training_data = CustomDataset(labels = adata.obs[\"res.0.8\"][train_indices], data = training_matrix.astype(float), \n",
    "                              transform= torch.from_numpy, target_transform =None)\n",
    "\n",
    "test_data = CustomDataset(labels = adata.obs[\"res.0.8\"][test_indices], data = test_matrix.astype(float), \n",
    "                          transform= torch.from_numpy, target_transform =None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce3636",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d666053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features= 39739, out_features=20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f657ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            _, predicted = pred.max(1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    acc = (100*correct)\n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return acc, test_loss, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470a5c9",
   "metadata": {},
   "source": [
    "## Model training with k-fold cross vallidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbb9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    Resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "            \n",
    "def cross_val_train(dataset, learning_rate):\n",
    "    print('--------------------------------')\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    k_folds = 5\n",
    "    num_epochs = 5\n",
    "    batch_size = 64\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    # For fold results\n",
    "    results = {}\n",
    "    results_f1 = {}\n",
    "\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "        # Print\n",
    "        print(f'\\nFOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                          dataset, \n",
    "                          batch_size=64, sampler=train_subsampler)\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "                          dataset,\n",
    "                          batch_size=64, sampler=test_subsampler)\n",
    "        \n",
    "        # Initialize the model with random parameters\n",
    "        network = NeuralNetwork().to(\"cpu\")\n",
    "        network.apply(reset_weights)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, num_epochs):\n",
    "\n",
    "            # Iterate over the DataLoader for training data\n",
    "            train_loop(trainloader, network, loss_function, optimizer, batch_size)\n",
    "\n",
    "            # Saving the model\n",
    "            save_path = f'./model-fold-{fold}.pth'\n",
    "            torch.save(network.state_dict(), save_path)\n",
    "\n",
    "            # Iterate over the test data and generate predictions\n",
    "            test_accuracy, test_loss, f1 = test_loop(testloader, network, loss_function)\n",
    "            results[fold] = test_accuracy\n",
    "            results_f1[fold] = f1\n",
    "                \n",
    "            print(f'Epoch {epoch+1}, Loss: {test_loss}, Accuracy: {test_accuracy}, F1: {f1}')\n",
    "            \n",
    "    # Print fold results\n",
    "    print(f'\\nK-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average accuracy: {sum/len(results.items())} %')\n",
    "    \n",
    "    sum = 0.0\n",
    "    for key, value in results_f1.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average F1: {sum/len(results_f1.items())} %\\n')\n",
    "    \n",
    "    state_dict = network.state_dict()\n",
    "    return f1, state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91eeba",
   "metadata": {},
   "source": [
    "## learning rate optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d9c7ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Learning rate:  0.0001\n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.309708744287491, Accuracy: 1.0445962233828847, F1: 0.030632999797728155\n",
      "Epoch 2, Loss: 2.2877223193645477, Accuracy: 2.7320208919244675, F1: 0.10556369701262576\n",
      "Epoch 3, Loss: 2.2757404148578644, Accuracy: 2.611490558457212, F1: 0.11248163556184511\n",
      "Epoch 4, Loss: 2.2645259499549866, Accuracy: 2.6516673362796306, F1: 0.11662157225588297\n",
      "Epoch 5, Loss: 2.2544761896133423, Accuracy: 2.571313780634793, F1: 0.11505598529932079\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.298818737268448, Accuracy: 3.455202892728003, F1: 0.06641248258172996\n",
      "Epoch 2, Loss: 2.2644433975219727, Accuracy: 4.13820811570912, F1: 0.10829368498552594\n",
      "Epoch 3, Loss: 2.2183236181735992, Accuracy: 4.2185616713539575, F1: 0.1021299088632346\n",
      "Epoch 4, Loss: 2.169584423303604, Accuracy: 4.379268782643632, F1: 0.10841858872930529\n",
      "Epoch 5, Loss: 2.1266640424728394, Accuracy: 4.6203294495781435, F1: 0.1130691975566216\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.2787151634693146, Accuracy: 2.772197669746886, F1: 0.059530583067543885\n",
      "Epoch 2, Loss: 2.2488086223602295, Accuracy: 3.173965447971073, F1: 0.07908671032121015\n",
      "Epoch 3, Loss: 2.225940555334091, Accuracy: 3.5757332261952595, F1: 0.09195400809082019\n",
      "Epoch 4, Loss: 2.2064581513404846, Accuracy: 3.696263559662515, F1: 0.0962901034163333\n",
      "Epoch 5, Loss: 2.1965813636779785, Accuracy: 4.13820811570912, F1: 0.10563796724765345\n",
      "\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.185729056596756, Accuracy: 4.45962233828847, F1: 0.16072274185270086\n",
      "Epoch 2, Loss: 2.1161941289901733, Accuracy: 4.781036560867818, F1: 0.18424269611514257\n",
      "Epoch 3, Loss: 2.071148067712784, Accuracy: 4.660506227400562, F1: 0.18562489991931275\n",
      "Epoch 4, Loss: 2.041331633925438, Accuracy: 4.7408597830454005, F1: 0.19186969318082672\n",
      "Epoch 5, Loss: 2.0140085220336914, Accuracy: 4.901566894335074, F1: 0.20622596654595587\n",
      "\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.2613622844219208, Accuracy: 2.6918441141020493, F1: 0.100585113623975\n",
      "Epoch 2, Loss: 2.221591293811798, Accuracy: 3.8569706709521894, F1: 0.1359944831184112\n",
      "Epoch 3, Loss: 2.184440404176712, Accuracy: 4.379268782643632, F1: 0.15639129671585275\n",
      "Epoch 4, Loss: 2.1551865339279175, Accuracy: 5.3836882282040985, F1: 0.2193873036712893\n",
      "Epoch 5, Loss: 2.1304423213005066, Accuracy: 5.6649256729610284, F1: 0.22573950760394973\n",
      "\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 2.571313780634793 %\n",
      "Fold 1: 4.6203294495781435 %\n",
      "Fold 2: 4.13820811570912 %\n",
      "Fold 3: 4.901566894335074 %\n",
      "Fold 4: 5.6649256729610284 %\n",
      "Average accuracy: 4.379268782643632 %\n",
      "Fold 0: 0.11505598529932079 %\n",
      "Fold 1: 0.1130691975566216 %\n",
      "Fold 2: 0.10563796724765345 %\n",
      "Fold 3: 0.20622596654595587 %\n",
      "Fold 4: 0.22573950760394973 %\n",
      "Average F1: 0.1531457248507003 %\n",
      "\n",
      "--------------------------------\n",
      "Learning rate:  0.001\n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.313841313123703, Accuracy: 2.2900763358778624, F1: 0.06210307614581307\n",
      "Epoch 2, Loss: 2.327992796897888, Accuracy: 4.539975893933307, F1: 0.16971302174691716\n",
      "Epoch 3, Loss: 2.0888408571481705, Accuracy: 7.39252711932503, F1: 0.3191998373351595\n",
      "Epoch 4, Loss: 2.024706870317459, Accuracy: 5.343511450381679, F1: 0.20795166001010681\n",
      "Epoch 5, Loss: 1.8652546256780624, Accuracy: 8.638007231820009, F1: 0.3722867093598201\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.017889603972435, Accuracy: 5.303334672559261, F1: 0.1351590100020532\n",
      "Epoch 2, Loss: 1.9203440845012665, Accuracy: 5.102450783447168, F1: 0.16908215773700594\n",
      "Epoch 3, Loss: 2.0097116976976395, Accuracy: 5.142627561269586, F1: 0.13063541209748042\n",
      "Epoch 4, Loss: 1.8251037150621414, Accuracy: 5.946163117717959, F1: 0.22414107967679575\n",
      "Epoch 5, Loss: 1.9848417043685913, Accuracy: 5.182804339092005, F1: 0.13757886210047796\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.159726917743683, Accuracy: 4.178384893531539, F1: 0.13809303194477615\n",
      "Epoch 2, Loss: 1.9874987751245499, Accuracy: 6.1872237846524705, F1: 0.2447583095865667\n",
      "Epoch 3, Loss: 1.867842823266983, Accuracy: 7.071112896745681, F1: 0.2663064997202596\n",
      "Epoch 4, Loss: 1.7223412841558456, Accuracy: 9.079951787866614, F1: 0.3576298611742374\n",
      "Epoch 5, Loss: 1.6412550956010818, Accuracy: 9.44154278826838, F1: 0.41178895030740026\n",
      "\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 1.954594925045967, Accuracy: 5.705102450783447, F1: 0.24000032750388336\n",
      "Epoch 2, Loss: 1.9039923250675201, Accuracy: 4.6203294495781435, F1: 0.14545822857027502\n",
      "Epoch 3, Loss: 1.8614224642515182, Accuracy: 5.745279228605866, F1: 0.20363979355007145\n",
      "Epoch 4, Loss: 1.8278668373823166, Accuracy: 5.544395339493772, F1: 0.204984883825912\n",
      "Epoch 5, Loss: 1.6577664613723755, Accuracy: 7.794294897549217, F1: 0.3791109281365974\n",
      "\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.0563702285289764, Accuracy: 6.548814785054239, F1: 0.28477533513320713\n",
      "Epoch 2, Loss: 1.9496168196201324, Accuracy: 7.4728806749698675, F1: 0.28629818242867056\n",
      "Epoch 3, Loss: 1.7394098192453384, Accuracy: 8.758537565287265, F1: 0.3776803694308724\n",
      "Epoch 4, Loss: 1.6328119784593582, Accuracy: 8.597830453997588, F1: 0.40855040139564375\n",
      "Epoch 5, Loss: 1.6107556819915771, Accuracy: 8.356769787063078, F1: 0.3603037458992821\n",
      "\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 8.638007231820009 %\n",
      "Fold 1: 5.182804339092005 %\n",
      "Fold 2: 9.44154278826838 %\n",
      "Fold 3: 7.794294897549217 %\n",
      "Fold 4: 8.356769787063078 %\n",
      "Average accuracy: 7.882683808758538 %\n",
      "Fold 0: 0.3722867093598201 %\n",
      "Fold 1: 0.13757886210047796 %\n",
      "Fold 2: 0.41178895030740026 %\n",
      "Fold 3: 0.3791109281365974 %\n",
      "Fold 4: 0.3603037458992821 %\n",
      "Average F1: 0.33221383916071556 %\n",
      "\n",
      "--------------------------------\n",
      "Learning rate:  0.01\n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.997536614537239, Accuracy: 6.267577340297308, F1: 0.19324377971656745\n",
      "Epoch 2, Loss: 1.56981460750103, Accuracy: 8.035355564483728, F1: 0.33833786794316456\n",
      "Epoch 3, Loss: 1.7256812751293182, Accuracy: 7.8746484531940535, F1: 0.3123420283383784\n",
      "Epoch 4, Loss: 1.4585669189691544, Accuracy: 7.955002008838891, F1: 0.3492464948935673\n",
      "Epoch 5, Loss: 1.6736319810152054, Accuracy: 8.115709120128566, F1: 0.33492452408212664\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.9739395678043365, Accuracy: 5.142627561269586, F1: 0.14770797207418465\n",
      "Epoch 2, Loss: 1.6975446492433548, Accuracy: 6.106870229007633, F1: 0.21806208403695138\n",
      "Epoch 3, Loss: 1.7697659581899643, Accuracy: 6.106870229007633, F1: 0.21067860971724228\n",
      "Epoch 4, Loss: 1.9928192049264908, Accuracy: 6.347930895942146, F1: 0.24429153845581741\n",
      "Epoch 5, Loss: 1.6243691146373749, Accuracy: 7.3523503415026115, F1: 0.32427835138514693\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.0851817280054092, Accuracy: 5.182804339092005, F1: 0.17237296154546417\n",
      "Epoch 2, Loss: 1.8995198011398315, Accuracy: 6.106870229007633, F1: 0.20689480887932793\n",
      "Epoch 3, Loss: 1.7482994496822357, Accuracy: 6.749698674166332, F1: 0.2549775330159309\n",
      "Epoch 4, Loss: 1.6044016927480698, Accuracy: 7.9951787866613095, F1: 0.3185368154234562\n",
      "Epoch 5, Loss: 1.6038974672555923, Accuracy: 8.115709120128566, F1: 0.3142702832163177\n",
      "\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.496628552675247, Accuracy: 3.9775010044194454, F1: 0.1271700542875451\n",
      "Epoch 2, Loss: 1.8588329255580902, Accuracy: 7.4728806749698675, F1: 0.31110127305970114\n",
      "Epoch 3, Loss: 1.881817877292633, Accuracy: 7.151466452390519, F1: 0.29711705380347836\n",
      "Epoch 4, Loss: 1.545901894569397, Accuracy: 8.115709120128566, F1: 0.3245120398988588\n",
      "Epoch 5, Loss: 1.7853594273328781, Accuracy: 7.593411008437123, F1: 0.31201917317708366\n",
      "\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=39739, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=20, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
      "Epoch 1, Loss: 2.287314385175705, Accuracy: 4.6203294495781435, F1: 0.13901296650875886\n",
      "Epoch 2, Loss: 3.632472515106201, Accuracy: 3.5355564483728403, F1: 0.09369996177198418\n",
      "Epoch 3, Loss: 1.6940251737833023, Accuracy: 7.071112896745681, F1: 0.27145439388628806\n",
      "Epoch 4, Loss: 1.998072326183319, Accuracy: 6.629168340699076, F1: 0.24370188685358532\n",
      "Epoch 5, Loss: 2.025779590010643, Accuracy: 5.90598633989554, F1: 0.23069063622931604\n",
      "\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 8.115709120128566 %\n",
      "Fold 1: 7.3523503415026115 %\n",
      "Fold 2: 8.115709120128566 %\n",
      "Fold 3: 7.593411008437123 %\n",
      "Fold 4: 5.90598633989554 %\n",
      "Average accuracy: 7.4166331860184815 %\n",
      "Fold 0: 0.33492452408212664 %\n",
      "Fold 1: 0.32427835138514693 %\n",
      "Fold 2: 0.3142702832163177 %\n",
      "Fold 3: 0.31201917317708366 %\n",
      "Fold 4: 0.23069063622931604 %\n",
      "Average F1: 0.3032365936179982 %\n",
      "\n",
      "Best learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-4, 1e-3, 1e-2]\n",
    "best_performance = 0.0\n",
    "\n",
    "# If current model has better performance, update best_model_state_dict\n",
    "for lr in learning_rates:\n",
    "    f1, state_dict = cross_val_train(training_data, lr)\n",
    "\n",
    "    if f1 > best_performance:\n",
    "        best_lr = lr\n",
    "        best_performance = f1\n",
    "        best_model_state_dict = state_dict\n",
    "print(\"Best learning rate: \", best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff4405",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05521460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = NeuralNetwork()\n",
    "best_model.load_state_dict(best_model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2142b",
   "metadata": {},
   "source": [
    "## Evaluate the final model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a444d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 37.6206\n",
      "Test Accuracy: 1.65%\n",
      "Test F1 Score: 0.3140\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                          test_data,\n",
    "                          batch_size=64)\n",
    "test_loss, test_accuracy, test_f1 = test_loop(test_dataloader, best_model, nn.CrossEntropyLoss())\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\\nTest Accuracy: {test_accuracy:.2f}%\\nTest F1 Score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
